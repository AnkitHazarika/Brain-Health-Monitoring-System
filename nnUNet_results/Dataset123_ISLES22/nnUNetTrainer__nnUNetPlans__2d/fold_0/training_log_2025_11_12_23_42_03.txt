
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-11-12 23:42:04.652125: do_dummy_2d_data_aug: False 
2025-11-12 23:42:04.657037: Using splits from existing split file: D:\Capstone\Experiment 3\nnUNet_preprocessed\Dataset123_ISLES22\splits_final.json 
2025-11-12 23:42:04.657656: The split file contains 5 splits. 
2025-11-12 23:42:04.657656: Desired fold for training: 0 
2025-11-12 23:42:04.657656: This split has 200 training and 50 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 266, 'patch_size': [112, 112], 'median_image_size_in_voxels': [112.0, 112.0], 'spacing': [2.0, 2.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 5, 'features_per_stage': [32, 64, 128, 256, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset123_ISLES22', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.0, 2.0, 2.0], 'original_median_shape_after_transp': [72, 112, 112], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2117.0, 'mean': 374.9803381258754, 'median': 356.0013427734375, 'min': 6.620041403948562e-06, 'percentile_00_5': 89.99862670898438, 'percentile_99_5': 878.9948120117188, 'std': 156.9442956212566}, '1': {'max': 4506.12158203125, 'mean': 222.5365358402489, 'median': 0.5183995962142944, 'min': -319.0020751953125, 'percentile_00_5': 0.00027764352853409946, 'percentile_99_5': 1673.6614990234375, 'std': 375.24569493155883}, '2': {'max': 5049.42724609375, 'mean': 1166.910578771508, 'median': 1260.6309814453125, 'min': -575.469482421875, 'percentile_00_5': 71.0, 'percentile_99_5': 2330.02392578125, 'std': 570.0462245514832}}} 
 
2025-11-12 23:42:30.112095: unpacking dataset... 
2025-11-12 23:42:31.768113: unpacking done... 
2025-11-12 23:42:31.775235: Unable to plot network architecture: 
2025-11-12 23:42:31.776249: No module named 'hiddenlayer' 
2025-11-12 23:42:31.835684:  
2025-11-12 23:42:31.835684: Epoch 92 
2025-11-12 23:42:31.836675: Current learning rate: 0.00574 
2025-11-13 00:18:02.218204: train_loss -0.936 
2025-11-13 00:18:02.221213: val_loss -0.828 
2025-11-13 00:18:02.221213: Pseudo dice [0.8446] 
2025-11-13 00:18:02.222201: Epoch time: 2130.38 s 
2025-11-13 00:18:04.139179:  
2025-11-13 00:18:04.139179: Epoch 93 
2025-11-13 00:18:04.140190: Current learning rate: 0.0057 
2025-11-13 00:50:21.752979: train_loss -0.9365 
2025-11-13 00:50:21.753981: val_loss -0.8293 
2025-11-13 00:50:21.753981: Pseudo dice [0.8467] 
2025-11-13 00:50:21.754983: Epoch time: 1937.63 s 
2025-11-13 00:50:23.402081:  
2025-11-13 00:50:23.402081: Epoch 94 
2025-11-13 00:50:23.402081: Current learning rate: 0.00565 
2025-11-13 01:22:40.742144: train_loss -0.9365 
2025-11-13 01:22:40.743144: val_loss -0.8252 
2025-11-13 01:22:40.743144: Pseudo dice [0.8427] 
2025-11-13 01:22:40.744144: Epoch time: 1937.34 s 
2025-11-13 01:22:43.022278:  
2025-11-13 01:22:43.023297: Epoch 95 
2025-11-13 01:22:43.023297: Current learning rate: 0.0056 
2025-11-13 01:55:00.714382: train_loss -0.9373 
2025-11-13 01:55:00.715375: val_loss -0.8231 
2025-11-13 01:55:00.716379: Pseudo dice [0.8415] 
2025-11-13 01:55:00.716379: Epoch time: 1937.69 s 
2025-11-13 01:55:02.388172:  
2025-11-13 01:55:02.388172: Epoch 96 
2025-11-13 01:55:02.389178: Current learning rate: 0.00555 
2025-11-13 02:27:19.803292: train_loss -0.9374 
2025-11-13 02:27:19.803292: val_loss -0.8327 
2025-11-13 02:27:19.804804: Pseudo dice [0.8491] 
2025-11-13 02:27:19.805819: Epoch time: 1937.42 s 
2025-11-13 02:27:21.443952:  
2025-11-13 02:27:21.444951: Epoch 97 
2025-11-13 02:27:21.444951: Current learning rate: 0.0055 
2025-11-13 02:59:38.898118: train_loss -0.9363 
2025-11-13 02:59:38.899117: val_loss -0.8254 
2025-11-13 02:59:38.899117: Pseudo dice [0.843] 
2025-11-13 02:59:38.900627: Epoch time: 1937.46 s 
2025-11-13 02:59:40.546135:  
2025-11-13 02:59:40.546135: Epoch 98 
2025-11-13 02:59:40.546135: Current learning rate: 0.00546 
2025-11-13 03:31:57.999056: train_loss -0.9368 
2025-11-13 03:31:58.000056: val_loss -0.8229 
2025-11-13 03:31:58.000056: Pseudo dice [0.8414] 
2025-11-13 03:31:58.001054: Epoch time: 1937.46 s 
2025-11-13 03:31:59.672565:  
2025-11-13 03:31:59.673586: Epoch 99 
2025-11-13 03:31:59.673586: Current learning rate: 0.00541 
2025-11-13 04:04:17.416919: train_loss -0.9383 
2025-11-13 04:04:17.417922: val_loss -0.8271 
2025-11-13 04:04:17.418917: Pseudo dice [0.844] 
2025-11-13 04:04:17.418917: Epoch time: 1937.74 s 
2025-11-13 04:04:18.989065:  
2025-11-13 04:04:18.989065: Epoch 100 
2025-11-13 04:04:18.989065: Current learning rate: 0.00536 
2025-11-13 04:36:36.406528: train_loss -0.9371 
2025-11-13 04:36:36.408036: val_loss -0.8342 
2025-11-13 04:36:36.408036: Pseudo dice [0.8517] 
2025-11-13 04:36:36.409048: Epoch time: 1937.42 s 
2025-11-13 04:36:38.211017:  
2025-11-13 04:36:38.211017: Epoch 101 
2025-11-13 04:36:38.212017: Current learning rate: 0.00531 
2025-11-13 05:08:55.571038: train_loss -0.9366 
2025-11-13 05:08:55.571038: val_loss -0.8273 
2025-11-13 05:08:55.572050: Pseudo dice [0.8453] 
2025-11-13 05:08:55.573053: Epoch time: 1937.36 s 
2025-11-13 05:08:57.200894:  
2025-11-13 05:08:57.200894: Epoch 102 
2025-11-13 05:08:57.201904: Current learning rate: 0.00526 
2025-11-13 05:41:14.788070: train_loss -0.938 
2025-11-13 05:41:14.788070: val_loss -0.8282 
2025-11-13 05:41:14.789062: Pseudo dice [0.8456] 
2025-11-13 05:41:14.790062: Epoch time: 1937.59 s 
2025-11-13 05:41:16.349198:  
2025-11-13 05:41:16.349198: Epoch 103 
2025-11-13 05:41:16.349198: Current learning rate: 0.00521 
2025-11-13 06:13:34.073733: train_loss -0.9381 
2025-11-13 06:13:34.073733: val_loss -0.8298 
2025-11-13 06:13:34.074729: Pseudo dice [0.8476] 
2025-11-13 06:13:34.074729: Epoch time: 1937.73 s 
2025-11-13 06:13:35.668084:  
2025-11-13 06:13:35.668084: Epoch 104 
2025-11-13 06:13:35.669083: Current learning rate: 0.00517 
2025-11-13 06:45:53.260108: train_loss -0.9387 
2025-11-13 06:45:53.260108: val_loss -0.8228 
2025-11-13 06:45:53.261107: Pseudo dice [0.8411] 
2025-11-13 06:45:53.261107: Epoch time: 1937.59 s 
2025-11-13 06:45:54.851192:  
2025-11-13 06:45:54.852703: Epoch 105 
2025-11-13 06:45:54.852703: Current learning rate: 0.00512 
2025-11-13 07:18:12.618144: train_loss -0.9394 
2025-11-13 07:18:12.619139: val_loss -0.8313 
2025-11-13 07:18:12.620143: Pseudo dice [0.8474] 
2025-11-13 07:18:12.621141: Epoch time: 1937.77 s 
2025-11-13 07:18:14.257807:  
2025-11-13 07:18:14.258829: Epoch 106 
2025-11-13 07:18:14.258829: Current learning rate: 0.00507 
2025-11-13 07:50:31.968020: train_loss -0.9381 
2025-11-13 07:50:31.969037: val_loss -0.8281 
2025-11-13 07:50:31.969037: Pseudo dice [0.8453] 
2025-11-13 07:50:31.970034: Epoch time: 1937.71 s 
2025-11-13 07:50:33.850000:  
2025-11-13 07:50:33.850000: Epoch 107 
2025-11-13 07:50:33.850000: Current learning rate: 0.00502 
2025-11-13 08:22:51.828754: train_loss -0.9383 
2025-11-13 08:22:51.830266: val_loss -0.8176 
2025-11-13 08:22:51.831277: Pseudo dice [0.8362] 
2025-11-13 08:22:51.831277: Epoch time: 1937.98 s 
2025-11-13 08:22:53.486580:  
2025-11-13 08:22:53.487584: Epoch 108 
2025-11-13 08:22:53.487584: Current learning rate: 0.00497 
2025-11-13 08:55:11.401656: train_loss -0.9394 
2025-11-13 08:55:11.402661: val_loss -0.8274 
2025-11-13 08:55:11.403664: Pseudo dice [0.8451] 
2025-11-13 08:55:11.403664: Epoch time: 1937.92 s 
2025-11-13 08:55:13.039989:  
2025-11-13 08:55:13.040988: Epoch 109 
2025-11-13 08:55:13.040988: Current learning rate: 0.00492 
2025-11-13 09:27:30.773264: train_loss -0.9386 
2025-11-13 09:27:30.774272: val_loss -0.823 
2025-11-13 09:27:30.774272: Pseudo dice [0.8407] 
2025-11-13 09:27:30.775792: Epoch time: 1937.73 s 
2025-11-13 09:27:32.402420:  
2025-11-13 09:27:32.403414: Epoch 110 
2025-11-13 09:27:32.403414: Current learning rate: 0.00487 
2025-11-13 09:59:50.147745: train_loss -0.9387 
2025-11-13 09:59:50.148761: val_loss -0.8215 
2025-11-13 09:59:50.149757: Pseudo dice [0.8398] 
2025-11-13 09:59:50.149757: Epoch time: 1937.75 s 
2025-11-13 09:59:51.816089:  
2025-11-13 09:59:51.817112: Epoch 111 
2025-11-13 09:59:51.817112: Current learning rate: 0.00483 
